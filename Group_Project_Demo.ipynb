{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ducnguyy/code/blob/master/Group_Project_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLjqk4yfXAem"
      },
      "source": [
        "Import necessary libraries, packages, and modules\n"
      ],
      "id": "JLjqk4yfXAem"
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb\n",
        "import gc"
      ],
      "metadata": {
        "id": "aDlBYIt7rOh5"
      },
      "id": "aDlBYIt7rOh5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33dbb554-3240-4b9e-aaf4-107880f38e7f"
      },
      "outputs": [],
      "source": [
        "#importing necessary packages for image processing\n",
        "from __future__ import print_function, division\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torchvision import transforms, utils\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import CIFAR100\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score,roc_auc_score\n",
        "\n",
        "from sklearn.utils import compute_class_weight\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import roc_curve"
      ],
      "id": "33dbb554-3240-4b9e-aaf4-107880f38e7f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScSVyxKWYLqF"
      },
      "source": [
        "Define necessary functions to:"
      ],
      "id": "ScSVyxKWYLqF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oKAuFVlYDlS"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "load the raw data\n",
        "'''\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "separate & merge data into in-use training dataset\n",
        "'''\n",
        "def create_superclass_labels_based_set(x):\n",
        "    set=[]\n",
        "    for i in range(len(superclass_label)):\n",
        "        if superclass_label[i]==x:\n",
        "            set.append(i)\n",
        "    return set\n",
        "\n",
        "def create_class_labels_based_set(x):\n",
        "    set=[]\n",
        "    for i in range(len(class_label)):\n",
        "        if class_label[i]==x:\n",
        "            set.append(i)\n",
        "    return set\n",
        "\n",
        "def divide_and_merge_class_labels_set(x,y,p):\n",
        "    get_num_set1=len(x)\n",
        "    get_num_set2=len(y)\n",
        "    total_num_instances=get_num_set1+get_num_set2\n",
        "    separate_milestone=round(total_num_instances*p)\n",
        "    separate_x=x[:separate_milestone] \n",
        "    separate_y=y[separate_milestone:] \n",
        "    return separate_x+separate_y\n",
        "\n",
        "def find_class_name(dataset,index_x):\n",
        "    return class_name[class_label[dataset[index_x]]]\n",
        "\n",
        "def find_superclass_name(dataset,index_x):\n",
        "    return superclass_name[superclass_label[dataset[index_x]]]\n",
        "\n",
        "def find_class_label(dataset,index_x):\n",
        "    return class_label[dataset[index_x]]\n",
        "\n",
        "def find_superclass_label(dataset,index_x):\n",
        "    return superclass_label[dataset[index_x]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "test if the data is in the right form\n",
        "'''\n",
        "def test_image(ID_test, dataset_name):\n",
        "  ID_test = 300\n",
        "  image = X_operator[ID_test]\n",
        "  if dataset_name == \"tree_1\":\n",
        "    plt.imshow(image) \n",
        "    plt.title(\"Coarse Label Name:{} \\n Fine Label Name:{}\"\n",
        "            .format(superclass_name[superclass_label[tree_1[ID_test]]], class_name[class_label[tree_1[ID_test]]]))\n",
        "    plt.show()\n",
        "  elif dataset_name == \"tree_2\":\n",
        "    plt.imshow(image) \n",
        "    plt.title(\"Coarse Label Name:{} \\n Fine Label Name:{}\"\n",
        "            .format(superclass_name[superclass_label[tree_2[ID_test]]], class_name[class_label[tree_2[ID_test]]]))\n",
        "    plt.show()\n",
        "  else:\n",
        "    plt.imshow(image) \n",
        "    plt.title(\"Coarse Label Name:{} \\n Fine Label Name:{}\"\n",
        "            .format(superclass_name[superclass_label[household[ID_test]]], class_name[class_label[household[ID_test]]]))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Change computing location from CPU to GPU if available\n",
        "'''\n",
        "def set_computing_location():\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  print(f\"Using {device} device\")\n",
        "  return device\n",
        "\n",
        "device = set_computing_location()\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "visualize the results\n",
        "'''\n",
        "def plotting(epochs, mean_loss, mean_f1, mean_auc, mean_accs):\n",
        "  n=[]\n",
        "  for i in range(epochs):\n",
        "    n=np.append(n,i)\n",
        "  #print(mean_loss)\n",
        "#   f = plt.figure()\n",
        "#   f.set_figwidth(10)\n",
        "#   f.set_figheight(7.5)\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Mean Loss\")\n",
        "  plt.plot(n, mean_loss)\n",
        "  plt.title(\"Loss regressed over epochs\")\n",
        "  plt.show()\n",
        "  #if the plot come out weird -> not a problem! It's because value of both regress up or down while training!\n",
        "  #plt.plot(mean_loss, mean_f1)\n",
        "  #plt.xlabel(\"Mean Loss\")\n",
        "  #plt.ylabel(\"Mean F1 score\")\n",
        "  #plt.title(\"Mean Loss vs Mean F1 score\")\n",
        "  #plt.show()\n",
        "\n",
        "  plt.plot(n, mean_accs)\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Mean Accuracy\")\n",
        "  plt.title(\"Accuracy over epochs\")\n",
        "  plt.show()\n",
        "\n",
        "  #plt.plot(mean_loss, mean_auc)\n",
        "  #plt.xlabel(\"Mean Loss\")\n",
        "  #plt.ylabel(\"Mean ROC-AUC score\")\n",
        "  #plt.title(\"Mean Loss vs Mean ROC-AUC score\")\n",
        "  #plt.show()\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ],
      "id": "0oKAuFVlYDlS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in3hinJdZVAn"
      },
      "source": [
        "\n",
        "Unwrap training data and transform it:\n",
        "\n",
        "Result: got X_train, X_test as training data and testing data respectively\n",
        "\n"
      ],
      "id": "in3hinJdZVAn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9505db40-4311-40d8-a5b8-a0896fc715cf"
      },
      "outputs": [],
      "source": [
        "cifar_train = CIFAR100(os.getcwd(), download= True, transform=transform)\n",
        "cifar_test = CIFAR100(os.getcwd(), train = False, transform=transform)\n",
        "\n",
        "test_data = unpickle('cifar-100-python/test')\n",
        "meta_data = unpickle('cifar-100-python/meta')\n",
        "train_data = unpickle('cifar-100-python/train')\n",
        "\n",
        "X_train=train_data[b'data']\n",
        "X_train = X_train.reshape(len(X_train),3,32,32)\n",
        "X_train = X_train.transpose(0,2,3,1)\n",
        "\n",
        "X_test=test_data[b'data']\n",
        "X_test = X_test.reshape(len(X_test),3,32,32)\n",
        "X_test = X_test.transpose(0,2,3,1)"
      ],
      "id": "9505db40-4311-40d8-a5b8-a0896fc715cf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOnmoHxncAyc"
      },
      "source": [
        "Create a class for Dataset with 3 initial functions: init, len, getitem"
      ],
      "id": "qOnmoHxncAyc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2Dmlb7qZRfc"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, X,label_set,dataset_name):\n",
        "        self.X=X \n",
        "        self.labels=np.array([])\n",
        "        self.super_labels=np.array([])\n",
        "        self.label_set=label_set \n",
        "        self.name=dataset_name\n",
        "        for example in range(self.X.shape[0]):\n",
        "          #create label set based on classes\n",
        "          if self.name == \"household\":\n",
        "            if superclass_label[self.label_set[example]]==5:\n",
        "                self.labels=np.append(self.labels,[1])\n",
        "            elif superclass_label[self.label_set[example]]==6:\n",
        "                self.labels=np.append(self.labels,[0]) \n",
        "          elif self.name == \"tree_1\":\n",
        "            if class_label[self.label_set[example]]==47:\n",
        "                self.labels=np.append(self.labels,[1])\n",
        "            elif class_label[self.label_set[example]]==52:\n",
        "                self.labels=np.append(self.labels,[0]) \n",
        "          elif self.name == \"tree_2\":\n",
        "            if class_label[self.label_set[example]]==56:\n",
        "                self.labels=np.append(self.labels,[1])\n",
        "            elif class_label[self.label_set[example]]==52:\n",
        "                self.labels=np.append(self.labels,[0])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "        \n",
        "    def __getitem__(self,ID):\n",
        "        label_of_img = self.labels[ID]\n",
        "        img_true=self.X[ID]\n",
        "        #this is the transforming step done from scratch\n",
        "        img_true=np.array(img_true)\n",
        "        img_true=img_true.astype(\"float64\")\n",
        "        img_true/=255\n",
        "        label_of_img=np.array(label_of_img)\n",
        "        img_true=torch.from_numpy(img_true)\n",
        "        img_true=img_true.permute(2,0,1)\n",
        "        label_of_img=torch.from_numpy(label_of_img)\n",
        "        return img_true, label_of_img"
      ],
      "id": "O2Dmlb7qZRfc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMstrYuKfxTx"
      },
      "source": [
        "(*)Define architectures of our model:\n",
        "\n",
        "Since the paper doesn't specify rigidly the activation functions in each layer, here we used the commonly choice of activation function: ReLU(). In the final layer, we used Sigmoid which is specified in the paper.  "
      ],
      "id": "zMstrYuKfxTx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "872f3413"
      },
      "outputs": [],
      "source": [
        "class CustomModel(nn.Module):\n",
        "    def __init__(self,dataset_name):\n",
        "        super(CustomModel,self).__init__()\n",
        "        \n",
        "        self.name=dataset_name\n",
        "\n",
        "        self.Flatten=nn.Flatten()\n",
        "\n",
        "        if self.name == \"household\":\n",
        "          self.features = nn.Sequential(\n",
        "              nn.Linear(in_features = 3072, out_features = 1000),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(in_features = 1000, out_features = 300),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(in_features = 300, out_features = 100),\n",
        "              nn.ReLU()\n",
        "          )\n",
        "          self.logit = nn.Sequential(\n",
        "              nn.Linear(in_features = 100, out_features = 1)\n",
        "          )\n",
        "          self.output = nn.Sequential(            \n",
        "              nn.Sigmoid()\n",
        "          )\n",
        "\n",
        "        elif self.name == \"tree_1\" or \"tree_2\":\n",
        "          self.features=nn.Sequential(\n",
        "            nn.Linear(in_features=3072,out_features=1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=1000,out_features=100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=100,out_features=10),\n",
        "            nn.ReLU()\n",
        "          )\n",
        "          self.logit = nn.Sequential(\n",
        "              nn.Linear(in_features = 10, out_features = 1)\n",
        "          )\n",
        "          self.output = nn.Sequential(\n",
        "              nn.Sigmoid()\n",
        "          )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.Flatten(x)\n",
        "        features = self.features(x)\n",
        "        logit = self.logit(features)\n",
        "        output = self.output(logit)\n",
        "        return features, logit, output"
      ],
      "id": "872f3413"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EId8i0FsfByd"
      },
      "source": [
        "Define loss criterion: here we compare between MSE, MFE(self-implemented), MSFE(self-implemented), Focal Loss, Weighted Cross Entropy Loss. "
      ],
      "id": "EId8i0FsfByd"
    },
    {
      "cell_type": "code",
      "source": [
        "#with mse/mfe/msfe we have output = output, target = target \n",
        "#with focal loss, we use both probs & output \n",
        "\n",
        "def MSE(output, target):\n",
        "  loss = ((1/2) * ((output - target) ** 2)).mean()\n",
        "  return loss\n",
        "\n",
        "def MFE(output, target):\n",
        "    pred_1 = []\n",
        "    true_1 = []\n",
        "    pred_2 = []\n",
        "    true_2 = []\n",
        "    for i in range(target.size(dim = 0)):\n",
        "      #minority and positive class\n",
        "      if target[i] == 1:\n",
        "        pred_1.append(output[i])\n",
        "        true_1.append(target[i])\n",
        "      else:\n",
        "      #majority and negative class\n",
        "        pred_2.append(output[i])\n",
        "        true_2.append(target[i])\n",
        "    if len(pred_1) == 0:\n",
        "      pred_2 = torch.cat(pred_2, dim = 0)\n",
        "      true_2 = torch.cat(true_2, dim = 0)\n",
        "      loss = MSE(pred_2, true_2) #FPE\n",
        "    elif len(pred_2) == 0:\n",
        "      pred_1 = torch.cat(pred_1, dim = 0)\n",
        "      true_1 = torch.cat(true_1, dim = 0)\n",
        "      loss = MSE(pred_1, true_1) #FNE\n",
        "    else:\n",
        "      pred_1 = torch.cat(pred_1, dim = 0)\n",
        "      true_1 = torch.cat(true_1, dim = 0)\n",
        "      pred_2 = torch.cat(pred_2, dim = 0)\n",
        "      true_2 = torch.cat(true_2, dim = 0)\n",
        "      loss = MSE(pred_2, true_2) + MSE(pred_1, true_1)\n",
        "    return loss\n",
        "\n",
        "def MSFE(output,target):\n",
        "    pred_1 = []\n",
        "    true_1 = []\n",
        "    pred_2 = []\n",
        "    true_2 = []\n",
        "    for i in range(target.size(dim = 0)):\n",
        "      if target[i] == 1:\n",
        "        pred_1.append(output[i])\n",
        "        true_1.append(target[i])\n",
        "      else:\n",
        "        pred_2.append(output[i])\n",
        "        true_2.append(target[i])\n",
        "    if len(pred_1) == 0:\n",
        "      pred_2 = torch.cat(pred_2, dim = 0)\n",
        "      true_2 = torch.cat(true_2, dim = 0)\n",
        "      FPE = MSE(pred_2,true_2)\n",
        "      loss = 1/2* ( (FPE ** 2) + (FPE ** 2))\n",
        "    elif len(pred_2) == 0:\n",
        "      pred_1 = torch.cat(pred_1, dim = 0)\n",
        "      true_1 = torch.cat(true_1, dim = 0)\n",
        "      FNE = MSE(pred_1,true_1)\n",
        "      loss = 1/2 * ( (FNE ** 2) + (( - FNE) ** 2))\n",
        "    else:\n",
        "      pred_1 = torch.cat(pred_1, dim = 0)\n",
        "      true_1 = torch.cat(true_1, dim = 0)\n",
        "      pred_2 = torch.cat(pred_2, dim = 0)\n",
        "      true_2 = torch.cat(true_2, dim = 0)\n",
        "      FPE = MSE(pred_2, true_2)\n",
        "      FNE = MSE(pred_1, true_1)\n",
        "      loss = (1/2) * (((FPE + FNE) ** 2) + ((FPE - FNE) ** 2))\n",
        "    return loss\n",
        "\n",
        "def sigmoid_focal_loss(inputs, targets, alpha: float = 0.25, gamma: float = 2):\n",
        "\n",
        "   prob = inputs.sigmoid()\n",
        "   ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
        "   p_t = prob * targets + (1 - prob) * (1 - targets)\n",
        "   loss = ce_loss * ((1 - p_t) ** gamma)\n",
        "\n",
        "   if alpha >= 0:\n",
        "       alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
        "       loss = alpha_t * loss\n",
        "\n",
        "   return loss.mean()\n"
      ],
      "metadata": {
        "id": "kD8DNptwAfzS"
      },
      "id": "kD8DNptwAfzS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbG37nmOlnko"
      },
      "source": [
        "# Function for training the model and making evaluation points:\n",
        "Here we use loss, accuracy, f-1 score, AUC score.\n",
        "NOTE: .to(device) is for running with available gpu, and is disabled by default. But run it in your machine pls."
      ],
      "id": "UbG37nmOlnko"
    },
    {
      "cell_type": "code",
      "source": [
        "def set_pipeline(X_train, dataset_name, imb_level, learning_rate, num_of_epochs, batch_size_train, shuffle_train, loss_name):\n",
        "#-------------------------------------------------------------------------------\n",
        "'''\n",
        "Making in-use dataset:\n",
        "#Step 1: create a set consisted of class labels.\n",
        "Parameter: p in [0,1]; p = Imb.Level\n",
        "'''\n",
        "#-------------------------------------------------------------------------------\n",
        "  house_device_set = create_superclass_labels_based_set(5)\n",
        "  furniture = create_superclass_labels_based_set(6)\n",
        "  household = divide_and_merge_class_labels_set(house_device_set, furniture, p = imb_level)\n",
        "\n",
        "  maple_tree = create_class_labels_based_set(47)\n",
        "  oak_tree = create_class_labels_based_set(52)\n",
        "  tree_1 = divide_and_merge_class_labels_set(maple_tree, oak_tree, p = imb_level)\n",
        "\n",
        "  palm_tree = create_class_labels_based_set(56)\n",
        "  oak_tree = create_class_labels_based_set(52)\n",
        "  tree_2 = divide_and_merge_class_labels_set(palm_tree, oak_tree, p = imb_level)\n",
        "#-------------------------------------------------------------------------------\n",
        "'''\n",
        "Step 2: Create X_operator.\n",
        "Distinguish it with X_train which includes not only data but also labels.\n",
        "'''\n",
        "#-------------------------------------------------------------------------------\n",
        "  if dataset_name == \"tree_1\":\n",
        "    X_operator = X_train[tree_1]\n",
        "    label = tree_1\n",
        "  elif dataset_name == \"tree_2\":\n",
        "    X_operator = X_train[tree_2]\n",
        "    label = tree_2\n",
        "  elif dataset_name == \"household\":\n",
        "    X_operator = X_train[household]\n",
        "    label = household\n",
        "  else:\n",
        "    print(\"Undefined Dataset.\")\n",
        "  print(f\"TRAINING SET'S: \\n Number of instance:\",X_operator.shape[0])\n",
        "#-------------------------------------------------------------------------------\n",
        "#Final step: making Dataset object and Dataloader:\n",
        "  CIFAR_operator = CustomDataset(X = X_operator, label_set = label, dataset_name = dataset_name)\n",
        "  Custom_DataLoader = torch.utils.data.DataLoader(CIFAR_operator, batch_size = batch_size_train, shuffle = shuffle_train, num_workers = 2)\n",
        "  model = CustomModel(dataset_name).float()#.to(device)\n",
        "\n",
        "#Define optimizer algorithm: here we used the most simple: Stochastic Gradient Descent\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "#-------------------------------------------------------------------------------\n",
        "  class_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(CIFAR_operator.labels), y = (CIFAR_operator.labels))\n",
        "  class_weights = torch.tensor(class_weights, dtype = torch.float)\n",
        "  \n",
        "  criterion_weighted = nn.CrossEntropyLoss(weight = class_weights)\n",
        "#-------------------------------------------------------------------------------\n",
        "  mean_loss = []\n",
        "  mean_f1 = []\n",
        "  mean_auc = []\n",
        "  mean_accs = []\n",
        "  for epoch in range(num_of_epochs):\n",
        "      loss_epochs = []\n",
        "      accs = []\n",
        "      f1_scores = []\n",
        "      aucs = []\n",
        "      for x,y in tqdm(Custom_DataLoader, disable = True): #due to the need of collecting a large amount of information, we temporarily hide the progress bar\n",
        "          #print(X.shape)\n",
        "          #move data to cuda for operating\n",
        "          x = x#.to(device)\n",
        "          y = torch.reshape(y,(-1,1))#.to(device)\n",
        "          features, logit, predict = model(x.float())\n",
        "#-------------------------------------------------------------------------------\n",
        "          if loss_name == \"MSE\":\n",
        "            loss = MSE(predict,y.float())\n",
        "          elif loss_name == \"MFE\":\n",
        "            loss = MFE(predict,y.float())\n",
        "          elif loss_name == \"MSFE\":\n",
        "            loss = MSFE(predict,y.float())\n",
        "          elif loss_name == \"WCE\":\n",
        "            predict_inverse = 1 - predict\n",
        "            predict_weighted = torch.cat((predict_inverse, predict), 1)\n",
        "            #print(predict_weighted.size())\n",
        "            loss = criterion_weighted(predict_weighted, y.squeeze().long()) \n",
        "          elif loss_name == \"FL\":\n",
        "            loss = sigmoid_focal_loss(logit, y.float())\n",
        "          else:\n",
        "            print(\"Undefined loss function formula.\")\n",
        "#-------------------------------------------------------------------------------           \n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          # print(loss.item())\n",
        "          loss_epochs.append(loss.item())\n",
        "\n",
        "          with torch.no_grad():\n",
        "              # compute the metrics for each batch\n",
        "              #for imb.20% thresh=0.426 \n",
        "              #for imb.10% thresh=0.22\n",
        "\n",
        "              predict_label = (predict > 0.426).float()\n",
        "              # pdb.set_trace()\n",
        "              acc = torch.mean((y == predict_label).float())\n",
        "              accs.append(acc)    \n",
        "              f1 = f1_score((y.cpu()).detach().numpy(), (predict_label.cpu()).detach().numpy())\n",
        "              f1_scores.append(f1)\n",
        "              if (1 in y) and (0 in y): \n",
        "                aucs.append(roc_auc_score(y.float().cpu(), predict.float().cpu()))\n",
        "              else:\n",
        "                aucs.append(np.array([0]))\n",
        "      #print(f\"Training Loss for current epoch\", epoch, \":\", np.mean(loss_epochs))\n",
        "      mean_loss.append(np.mean(loss_epochs).item())\n",
        "      #print(f\"Training Acc for current epoch\", epoch, \":\", np.mean(accs))\n",
        "      mean_accs.append(np.mean(accs).item())\n",
        "      #print(f\"Training F1 score for current epoch\", epoch, \":\", np.mean(f1_scores))\n",
        "      mean_f1.append(np.mean(f1_scores).item())\n",
        "      #print(f\"Training AUC score for current epoch\", epoch, \":\", np.mean(aucs))\n",
        "      mean_auc.append(np.mean(aucs).item())\n",
        "  #print(\"pred:{},y:{}\".format(predict,y))\n",
        "#-------------------------------------------------------------------------------  \n",
        "  print(\"\\nTRAIN RESULT with dataset: \", dataset_name, \" \\nimb_level = \",imb_level, \"\\nbatch_size = \", batch_size_train, \" \\nusing loss function: \", loss_name,\" \\nlearning_rate = \",learning_rate,\"\\nnumber of epoch: \",num_of_epochs)\n",
        "  print(\"Balancing Class Weight: \",class_weights)\n",
        "  plot = plotting(num_of_epochs, mean_loss, mean_f1, mean_auc, mean_accs)\n",
        "#-------------------------------------------------------------------------------\n",
        "  return model"
      ],
      "metadata": {
        "id": "YSk9Sjqq_LsJ"
      },
      "id": "YSk9Sjqq_LsJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function for testing:"
      ],
      "metadata": {
        "id": "Lz3lJHIx1SiJ"
      },
      "id": "Lz3lJHIx1SiJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AraKUr2MgefV"
      },
      "outputs": [],
      "source": [
        "def check_accuracy(model, loader):\n",
        "    model.eval()\n",
        "\n",
        "    overall_f1_score = np.array([])\n",
        "    overaucs = []\n",
        "    threshold_test = np.array([])\n",
        "    threshold = 1\n",
        "\n",
        "    while threshold > 0:\n",
        "\n",
        "      threshold -= 0.1\n",
        "      #print(\"thresh is:\", threshold)\n",
        "      threshold_test = np.append(threshold_test, threshold)\n",
        "\n",
        "      with torch.no_grad():\n",
        "\n",
        "          acc = []\n",
        "          f1_scores = [] \n",
        "          aucs = []\n",
        "\n",
        "          for x, y in loader:\n",
        "              #x = x.to(device=device)\n",
        "    #          x = np.array(x)\n",
        "    #          x = torch.from_numpy(x)\n",
        "              #x = x.permute(0,3,2,1)\n",
        "              \n",
        "              #y = y.to(device=device)\n",
        "              y = torch.reshape(y, (-1, 1))\n",
        "              features, logit, predict = model(x.float())\n",
        "              precision, recall, _ = precision_recall_curve((y.cpu()).detach().numpy(), (predict.cpu()).detach().numpy())\n",
        "              auc_score = auc(recall, precision)\n",
        "              #print(auc_score)\n",
        "              # compute the metrics for each batch\n",
        "              predict_label = (predict > threshold).float()\n",
        "              # pdb.set_trace()\n",
        "              #acc = torch.mean((y == predict_label).float())\n",
        "              #accs.append(acc)    \n",
        "              f1 = f1_score((y.cpu()).detach().numpy(), (predict_label.cpu()).detach().numpy())\n",
        "              f1_scores.append(f1)\n",
        "              \n",
        "              aucs.append(roc_auc_score(y.float(), predict))\n",
        "\n",
        "          #print(\"pred:{},y:{}\".format(predict_label,y))\n",
        "          #print(\"pred size:{},y size:{}\".format(predict_label.size(),y.size()))\n",
        "          #print(f\"Test Acc:\", np.mean(accs))\n",
        "          #print(f\"Test F1 score:\", np.mean(f1_scores))\n",
        "          overall_f1_score = np.append(overall_f1_score, np.mean(f1_scores))\n",
        "          #print(f\"Test AUC score:\", np.mean(aucs))\n",
        "          overaucs = np.append(overaucs, np.mean(aucs)) \n",
        "\n",
        "    maximum_f1 = 0\n",
        "    for i in overall_f1_score:\n",
        "      if maximum_f1 < i:\n",
        "        maximum_f1 = i\n",
        "    print(\"Max F1 Score: \", maximum_f1) \n",
        "    print(\"ROC-AUC Score: \", overaucs[-1])\n",
        "\n",
        "    plt.plot(threshold_test, overall_f1_score)\n",
        "    plt.xlabel(\"Threshold\")\n",
        "    plt.ylabel(\"Mean F1 score\")\n",
        "    plt.title(\"Mean F1 score over different thresholds\")\n",
        "    plt.show()\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(y+1 , predict.detach().numpy(), pos_label=2)\n",
        "    g=plt.plot(fpr,tpr)\n",
        "    plt.title(\"ROC-AUC Curve\")\n",
        "    plt.show()\n",
        "    model.train()\n",
        "    \n",
        "    return roc_curve(y.cpu().detach().numpy(),predict.cpu().detach().numpy())\n"
      ],
      "id": "AraKUr2MgefV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KHnIKuyobnY"
      },
      "outputs": [],
      "source": [
        "def test_phase(X_test, dataset_name, imb_level, batch_size):\n",
        "#-------------------------------------------------------------------------------\n",
        "  print(\"TEST RESULT with dataset \", dataset_name, \" imb_level = \",imb_level, \"batch_size = \", batch_size)\n",
        "  #Test to see the image: worked\n",
        "  #image = X_test[3]\n",
        "  #plt.imshow(image)\n",
        "  #plt.show() \n",
        "  #print(X_test.shape,type(X_test))\n",
        "#-------------------------------------------------------------------------------\n",
        "  house_device_set = create_superclass_labels_based_set(5)\n",
        "  furniture = create_superclass_labels_based_set(6)\n",
        "  household = divide_and_merge_class_labels_set(house_device_set, furniture, p = imb_level)\n",
        "\n",
        "  maple_tree = create_class_labels_based_set(47)\n",
        "  oak_tree = create_class_labels_based_set(52)\n",
        "  tree_1 = divide_and_merge_class_labels_set(maple_tree, oak_tree, p = imb_level)\n",
        "\n",
        "  palm_tree = create_class_labels_based_set(56)\n",
        "  oak_tree = create_class_labels_based_set(52)\n",
        "  tree_2 = divide_and_merge_class_labels_set(palm_tree, oak_tree, p = imb_level)\n",
        "#-------------------------------------------------------------------------------  \n",
        "  if dataset_name == \"tree_1\":\n",
        "    X_operator = X_test[tree_1]\n",
        "    label = tree_1\n",
        "  elif dataset_name == \"tree_2\":\n",
        "    X_operator = X_test[tree_2]\n",
        "    label = tree_2\n",
        "  else:\n",
        "    X_operator = X_test[household]\n",
        "    label = household\n",
        "  print(\"Shape of X_operator aka X_input: \",X_operator.shape)\n",
        "#-------------------------------------------------------------------------------\n",
        "  CIFAR_operator = CustomDataset(X = X_operator, label_set = label, dataset_name = dataset_name)\n",
        "  Custom_DataLoader = torch.utils.data.DataLoader(CIFAR_operator, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "  fpr, tpr, thresholds = check_accuracy(model, Custom_DataLoader)\n",
        "  print(\"Done!\")\n",
        "\n",
        "  delete_list = [CIFAR_operator, Custom_DataLoader, fpr, tpr, thresholds, class_label, superclass_label, class_name, superclass_name]\n",
        "  return delete_list  \n",
        "#-------------------------------------------------------------------------------"
      ],
      "id": "1KHnIKuyobnY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The autorun pipeline:\n",
        "Run it"
      ],
      "metadata": {
        "id": "tLbvQU4LD0mV"
      },
      "id": "tLbvQU4LD0mV"
    },
    {
      "cell_type": "code",
      "source": [
        "REMIND_1 = [\"MSE\", 'MFE\", \"MSFE\", \"FL\", \"WCE\"]\n",
        "REMIND_2 = [0.2, 0.1, 0.05]\n",
        "REMIND_3 = [\"tree_2\",\"household\", \"tree_1\"]\n",
        "\n",
        "for dataset_name_gen in REMIND_3:\n",
        "\n",
        "    for imb_level_gen in REMIND_2:\n",
        "\n",
        "      for loss_gen in REMIND_1:  \n",
        "\n",
        "        class_label = train_data[b'fine_labels']\n",
        "        superclass_label = train_data[b'coarse_labels']\n",
        "        class_name = meta_data[b'fine_label_names']\n",
        "        superclass_name = meta_data[b'coarse_label_names']\n",
        "\n",
        "        model = set_pipeline(X_train, dataset_name = dataset_name_gen, imb_level = imb_level_gen, learning_rate = 0.1, \n",
        "                     num_of_epochs = 100, batch_size_train = 128, shuffle_train = True, loss_name = loss_gen)\n",
        "        \n",
        "        class_label = test_data[b'fine_labels']\n",
        "        superclass_label = test_data[b'coarse_labels']\n",
        "        class_name = meta_data[b'fine_label_names']\n",
        "        superclass_name = meta_data[b'coarse_label_names']\n",
        "        \n",
        "        test_result = test_phase(X_test, dataset_name = dataset_name_gen, imb_level = imb_level_gen, \n",
        "                                 batch_size = X_test.shape[0])\n",
        "        \n",
        "        del test_result\n",
        "        del model \n",
        "        gc.collect()\n"
      ],
      "metadata": {
        "id": "6I9fm3VZytoy"
      },
      "id": "6I9fm3VZytoy",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}